{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.special import expit\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel,linear_kernel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_friedman1, make_friedman2, make_friedman3, load_boston\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_gaussian_quantiles, make_circles\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IRLS(X, y, delta=1e-4, n_iter=20, tol=1e-3):\n",
    "    n_samples, n_features = X.shape\n",
    "    # y.shape (n_samples, n_features) n_feature=1?\n",
    "    y=y.reshape((n_samples,))\n",
    "    #print(y.shape)\n",
    "    delta = delta * np.ones((y.shape))\n",
    "    w_init = 1\n",
    "    w = w_init * np.ones((y.shape)).reshape((-1,))\n",
    "    W = np.diag(w)\n",
    "    \n",
    "    #print(\"W shape\",W.shape)\n",
    "    #print(\"w shape\",w.shape)\n",
    "    beta = np.dot(np.linalg.inv(np.dot(np.dot(X.T,W),X)),np.dot(np.dot(X.T,W),y))\n",
    "    # beta shape \n",
    "    for i in range(n_iter):\n",
    "        #w_=w\n",
    "        print(\"iteration {}\".format(i))\n",
    "        beta_old=beta\n",
    "        w = 1/(np.maximum(delta,np.absolute(y-np.dot(X,beta)))).reshape((-1,))\n",
    "        W = np.diag(w)\n",
    "        #print(w.shape)\n",
    "        #print(W.shape)\n",
    "        beta = np.dot(np.linalg.inv(np.dot(np.dot(X.T,W),X)),np.dot(np.dot(X.T,W),y))\n",
    "        #print(\"beta shape\",beta.shape)\n",
    "        tolerance = np.sum(np.absolute(beta-beta_old))\n",
    "        print(\"tolerance is {}\".format(tolerance))\n",
    "        if tolerance < tol and i >1:\n",
    "            break\n",
    "    return beta.reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RVR(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel='rbf',\n",
    "        rbf_gamma=None,\n",
    "        n_iter=5000,\n",
    "        threshold_dif=1e-3,\n",
    "        alpha=1e-6,\n",
    "        beta=1.e-6,\n",
    "        threshold_alpha=1e9,\n",
    "        bias_used = True\n",
    "    ):\n",
    "        self.kernel = kernel\n",
    "        self.rbf_gamma = rbf_gamma\n",
    "        self.n_iter = n_iter\n",
    "        self.threshold_dif = threshold_dif\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.threshold_alpha = threshold_alpha\n",
    "        self.bias_used = bias_used\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            'kernel': self.kernel,\n",
    "            'rbf_gamma': self.rbf_gamma,\n",
    "            'n_iter': self.n_iter,\n",
    "            'threshold_dif': self.threshold_dif,\n",
    "            'alpha': self.alpha,\n",
    "            'beta': self.beta,\n",
    "            'threshold_alpha': self.threshold_alpha,\n",
    "            'bias_used': self.bias_used\n",
    "        }\n",
    "        return params\n",
    "  \n",
    "    def set_params(self, **params):\n",
    "        for parameter, value in params.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def apply_kernel(self, x, y):\n",
    "        if x.shape[1]!=y.shape[1]:\n",
    "            raise ValueError(\"Wrong shape of input\")\n",
    "        n_samples1, n_features=x.shape\n",
    "        n_samples2, n_features=y.shape\n",
    "        if self.kernel=='rbf':\n",
    "            #print(x.shape)\n",
    "            #print(y.shape)\n",
    "            phi = rbf_kernel(x, y, self.rbf_gamma)\n",
    "        \n",
    "        elif self.kernel=='linear':\n",
    "            phi = linear_kernel(x,y)\n",
    "        elif self.kernel=='linear_spline':\n",
    "            #print(\"x repeat shape\",np.repeat(x,n_samples2,axis=1).shape)\n",
    "            #print(\"y repeat shape\",np.repeat(y,n_samples1,axis=1).shape)\n",
    "            #a = np.repeat(x,3,axis=1)\n",
    "            #b=np.repeat(y.T,3,axis=0)\n",
    "            Min = np.minimum(np.repeat(x,n_samples2,axis=1),np.repeat(y.T,n_samples1,axis=0)) #( 100, 100)\n",
    "            #print(\"x shape: {}\".format(x.shape))\n",
    "            #print(\" shape: {}\".format(y.shape))\n",
    "            phi =1 + linear_kernel(x,y) + linear_kernel(x,y)*Min - (x+y.T)*np.square(Min)/2 + np.power(Min,3)/3\n",
    "            #output= 1 + np.dot(x,y)+ np.dot(x,y)*min(x,y)- ((x + y)/2)*(min(x,y))**2 + ((min(x,y))**3)/3\n",
    "            #print(np.amax(np.absolute (phi-output)))\n",
    "        else:\n",
    "            raise ValueError(\"rbf or linear kernel only!\")\n",
    "            \n",
    "        if self.bias_used:\n",
    "            phi = np.append(np.ones((phi.shape[0],1)), phi, axis=1)\n",
    "        return phi  \n",
    "        \n",
    "    def prune(self):\n",
    "        keep_alpha = self.alpha_ < self.threshold_alpha\n",
    "        #if not np.any(self.alpha_[keep_alpha]):\n",
    "        #    keep_alpha[0] = True\n",
    "        #print(\"keep_alpha shape: {}\".format(keep_alpha.shape))\n",
    "        \n",
    "        if self.bias_used:\n",
    "            if not keep_alpha[0]:\n",
    "                self.bias_used = False\n",
    "            self.relevance_ = self.relevance_[keep_alpha[1:]]\n",
    "        else:\n",
    "             self.relevance_ = self.relevance_[keep_alpha]\n",
    "        \n",
    "        self.alpha_ = self.alpha_[keep_alpha]\n",
    "        self.alpha_old = self.alpha_old[keep_alpha]\n",
    "        #self.beta_ = self.beta_[keep_alpha]\n",
    "        self.mu_ = self.mu_[keep_alpha]\n",
    "        self.gamma_ = self.gamma_[keep_alpha]\n",
    "        self.phi = self.phi[:,keep_alpha]\n",
    "        #self.sigma_ = self.sigma_[:,keep_alpha]\n",
    "        self.sigma_ = self.sigma_[np.ix_(keep_alpha,keep_alpha)]\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.relevance_ = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.phi = self.apply_kernel(X, X)\n",
    "        \n",
    "        n_basis_functions = self.phi.shape[1]  # if bias_used is True, n-basis_funtions = n_samples + 1 \n",
    "        \n",
    "       \n",
    "        self.mu_ = np.zeros(n_basis_functions)\n",
    "        self.alpha_ = self.alpha * np.ones(n_basis_functions)\n",
    "        self.beta_ = self.beta\n",
    "        #self.beta_ = np.ones(n_basis_functions) * (expit(np.dot(self.phi,self.w_mp_))/(1-expit(np.dot(self.phi,self.w_mp_))))\n",
    "        #print(sel.beta_.shape)\n",
    "        \n",
    "        \n",
    "        self.alpha_old = self.alpha_\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            self.sigma_ = np.linalg.inv(self.beta_ * np.dot(self.phi.T, self.phi) + np.diag(self.alpha_))\n",
    "            #print(\"sigma shape: {}\".format(self.sigma_.shape))\n",
    "            \n",
    "            self.mu_ = self.beta_ * np.dot(np.dot(self.sigma_,self.phi.T),self.y)\n",
    "            \n",
    "            #self.sigma_ = np.linalg.inv(np.dot(np.dot(self.phi.T, np.diag(self.beta_)),self.phi) + np.diag(self.alpha_))\n",
    "            #print(\"sigma shape: {}\".format(self.sigma_.shape))\n",
    "            \n",
    "            #self.w_mp_ = (np.dot(np.dot(self.sigma_,self.phi.T),np.diag(self.beta_)),self.y)\n",
    "            #print(\"mu shape: {}\".format(self.mu_.shape))\n",
    "            \n",
    "            self.gamma_ = 1 - self.alpha_ * np.diag(self.sigma_)\n",
    "            #print(\"gamma shape: {}\".format(self.gamma_.shape))\n",
    "            \n",
    "            self.alpha_ = self.gamma_/(self.mu_ ** 2)\n",
    "            #print(\"alpha shape: {}\".format(self.alpha_.shape))\n",
    "            \n",
    "            self.beta_ = (n_samples - np.sum(self.gamma_))/(np.sum((np.absolute(self.y - np.dot(self.phi, self.mu_)))**2))\n",
    "            #print(\"beta shape: {}\".format(self.beta_.shape))\n",
    "            \n",
    "            self.prune()\n",
    "            \n",
    "            \n",
    "            delta = np.amax(np.absolute(self.alpha_ - self.alpha_old))\n",
    "            if delta < self.threshold_dif and i > 1:\n",
    "                print(\"Iteration: {}\".format(i))\n",
    "                break\n",
    "                \n",
    "            self.alpha_old = self.alpha_\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.relevance_.shape)\n",
    "        phi = self.apply_kernel(X, self.relevance_)\n",
    "        #print(phi.shape)\n",
    "        y = np.dot(phi, self.mu_)\n",
    "        \n",
    "#         if eval_MSE:\n",
    "#             #print(\"sigma shape: {}\".format(self.sigma_.shape))\n",
    "#             #print(\"phi shape: {}\".format(self.phi.shape))\n",
    "            \n",
    "#             MSE = (1/self.beta_) + np.dot(phi, np.dot(self.sigma_, phi.T))\n",
    "#             return y, MSE[:, 0]\n",
    "#         else:\n",
    "#             return y\n",
    "        return y\n",
    "        \n",
    "class RVC(BaseEstimator, ClassifierMixin):    \n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel='rbf',\n",
    "        rbf_gamma=None,\n",
    "        n_iter=5000,\n",
    "        threshold_dif=1e-3,\n",
    "        alpha=1e-6,\n",
    "        beta=1.e-6,\n",
    "        threshold_alpha=1e9,\n",
    "        bias_used = False\n",
    "        ):\n",
    "        self.kernel = kernel\n",
    "        self.rbf_gamma = rbf_gamma\n",
    "        self.n_iter = n_iter\n",
    "        self.threshold_dif = threshold_dif\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.threshold_alpha = threshold_alpha\n",
    "        self.bias_used = bias_used\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            'kernel': self.kernel,\n",
    "            'rbf_gamma': self.rbf_gamma,\n",
    "            'n_iter': self.n_iter,\n",
    "            'threshold_dif': self.threshold_dif,\n",
    "            'alpha': self.alpha,\n",
    "            'beta': self.beta,\n",
    "            'threshold_alpha': self.threshold_alpha,\n",
    "            'bias_used': self.bias_used\n",
    "        }\n",
    "        return params\n",
    "  \n",
    "    def set_params(self, **params):\n",
    "        for parameter, value in params.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def apply_kernel(self, x, y):\n",
    "        n_samples1, n_features=x.shape # (100,2)\n",
    "        n_samples2, n_features=y.shape # (100,1)\n",
    "        # \n",
    "        if self.kernel=='rbf':\n",
    "            phi = rbf_kernel(x, y, self.rbf_gamma)\n",
    "        \n",
    "        elif self.kernel=='linear':\n",
    "            phi = linear_kernel(x,y)\n",
    "        elif self.kernel=='linear_spline':\n",
    "            Min = np.minimum(np.repeat(x,n_samples2,axis=1),np.repeat(y.T,n_samples1,axis=0)) #( 100, 100)\n",
    "            phi =1 + linear_kernel(x,y) + linear_kernel(x,y)*Min - (x+y.T)*np.square(Min)/2 + np.power(Min,3)/3\n",
    "        else:\n",
    "            raise ValueError(\"rbf or linear kernel only!\")\n",
    "           \n",
    "        if self.bias_used:\n",
    "            phi = np.append(np.ones((phi.shape[0],1)), phi, axis=1)\n",
    "        #print(\"phi has the shape of {}\".format(phi.shape))     \n",
    "        return phi  \n",
    "    \n",
    "    def prune(self):\n",
    "        keep_alpha = self.alpha_ < self.threshold_alpha\n",
    "        #if not np.any(self.alpha_[keep_alpha]):\n",
    "        #    keep_alpha[0] = True\n",
    "        #print(\"keep_alpha shape: {}\".format(keep_alpha.shape))\n",
    "        \n",
    "        if self.bias_used:\n",
    "            if not keep_alpha[0]:\n",
    "                self.bias_used = False\n",
    "            self.relevance_ = self.relevance_[keep_alpha[1:]]\n",
    "        else:\n",
    "             self.relevance_ = self.relevance_[keep_alpha]\n",
    "        \n",
    "        self.alpha_ = self.alpha_[keep_alpha]\n",
    "        self.alpha_old = self.alpha_old[keep_alpha]\n",
    "        #self.beta_ = self.beta_[keep_alpha]\n",
    "        self.w_mp_ = self.w_mp_[keep_alpha]\n",
    "        self.gamma_ = self.gamma_[keep_alpha]\n",
    "        self.phi = self.phi[:,keep_alpha]\n",
    "        #print(self.phi.shape)\n",
    "        #self.sigma_ = self.sigma_[:,keep_alpha]\n",
    "        self.sigma_ = self.sigma_[np.ix_(keep_alpha,keep_alpha)]\n",
    "        return self\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_samples, n_features = X.shape # n_samples=100 n_features=2 (100,2)\n",
    "        self.relevance_ = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.phi = self.apply_kernel(X, X)\n",
    "        \n",
    "        n_basis_functions = self.phi.shape[1]  # if bias_used is True, n-basis_funtions = n_samples + 1 \n",
    "        \n",
    "       \n",
    "        \n",
    "        self.alpha_ = self.alpha * np.ones(n_basis_functions)\n",
    "        #self.beta_ = self.beta\n",
    "        self.alpha_old = self.alpha_\n",
    "        \n",
    "        self.w_mp_ = np.zeros(n_basis_functions)\n",
    "        #print(\"w_mp has the shape of\",self.w_mp_.shape)\n",
    "        #expit(np.dot(self.phi, self.w_mp_))\n",
    "        #self.beta_ = np.ones(n_basis_functions)\n",
    "        self.beta_ = np.ones(n_basis_functions) * (expit(np.dot(self.phi,self.w_mp_))/(1-expit(np.dot(self.phi,self.w_mp_))))\n",
    "        for i in range(self.n_iter):\n",
    "            #print(self.phi.shape)\n",
    "            self.sigma_ = np.linalg.inv(np.dot(np.dot(self.phi.T, np.diag(self.beta_)),self.phi) + np.diag(self.alpha_))\n",
    "            #self.sigma_ = np.linalg.inv(self.beta_ * np.dot(self.phi.T, self.phi) + np.diag(self.alpha_))\n",
    "            #print(\"sigma shape: {}\".format(self.sigma_.shape))\n",
    "            self.w_mp_ = np.dot(np.dot(np.dot(self.sigma_,self.phi.T),np.diag(self.beta_)),self.y)\n",
    "            #self.mu_ = self.beta_ * np.dot(np.dot(self.sigma_,self.phi.T),self.y)\n",
    "            #print(\"mu shape: {}\".format(self.mu_.shape))\n",
    "            \n",
    "            self.gamma_ = 1 - self.alpha_ * np.diag(self.sigma_)\n",
    "            #print(\"gamma shape: {}\".format(self.gamma_.shape))\n",
    "            #print(\"w_mp has the shape of {}\".format(self.w_mp_.shape))\n",
    "            #tmp=np.square(self.w_mp_)\n",
    "            #print(\"w_mp has the shape of: {}\".format(tmp.shape))\n",
    "            self.alpha_ = self.gamma_/np.square(self.w_mp_)\n",
    "            #print(\"alpha shape: {}\".format(self.alpha_.shape))\n",
    "            \n",
    "            self.beta_ = (expit(np.dot(self.phi,self.w_mp_))/(1-expit(np.dot(self.phi,self.w_mp_))))\n",
    "            #self.beta_ = (n_samples - np.sum(self.gamma_))/(np.sum((np.absolute(self.y - np.dot(self.phi, self.w_mp_)))**2))\n",
    "            #print(\"beta shape: {}\".format(self.beta_.shape))\n",
    "            \n",
    "            self.prune()\n",
    "            \n",
    "            #delta = np.amax(np.absolute(self.beta_ - self.beta_old))\n",
    "            delta = np.amax(np.absolute(self.alpha_ - self.alpha_old))\n",
    "            if delta < self.threshold_dif and i > 1:\n",
    "                print(\"Iteration: {}\".format(i))\n",
    "                break\n",
    "                \n",
    "            self.alpha_old = self.alpha_\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        phi = self.apply_kernel(X, self.relevance_)\n",
    "        #print(phi.shape)\n",
    "        y = np.dot(phi, self.w_mp_)\n",
    "        \n",
    "        #print(\"y before sigmoid: \",y)\n",
    "        y=expit(y)\n",
    "        return y\n",
    "        #return np.column_stack((1-y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(N, sigma):\n",
    "    #X = np.sort(20 * np.random.rand(N, 1)-10, axis=0)\n",
    "    X=np.linspace(-10,10,N).reshape((N,1))\n",
    "   \n",
    "    y = np.sin(X)/X    # rbf kernel\n",
    "    #y = 2*X + 10       # linear kernel\n",
    "    y=y.ravel()\n",
    "    noise = np.random.normal(0, sigma, N)\n",
    "    #noise = 0\n",
    "    t = y + noise\n",
    "    #y[::5] += sigma * (0.5 - np.random.rand(int(N/5)))\n",
    "    #t=y+np.random.normal(0,0.2)\n",
    "    return X, t\n",
    "\n",
    "\n",
    "# N=100    # \n",
    "# The running time for RVM is 0.10185384750366211\n",
    "# The length of relevance vetor is : 9\n",
    "# The running time for SVM is 0.00142669677734375\n",
    "# The length of support vetor is : 62\n",
    "\n",
    "\n",
    "# N=200    \n",
    "# The running time for RVM is 0.8798322677612305\n",
    "# The length of relevance vetor is : 9\n",
    "# The running time for SVM is 0.0034551620483398438\n",
    "# The length of support vetor is : 122\n",
    "\n",
    "# N=300\n",
    "# The running time for RVM is 0.6392791271209717\n",
    "# The length of relevance vetor is : 9\n",
    "# The running time for SVM is 0.006904125213623047\n",
    "# The length of support vetor is : 190\n",
    "\n",
    "# N=400   \n",
    "# The running time for RVM is 1.4351470470428467\n",
    "# The length of relevance vetor is : 9\n",
    "# The running time for SVM is 0.012686967849731445\n",
    "# The length of support vetor is : 256\n",
    "\n",
    "# N=500\n",
    "# The running time for RVM is 0.8161935806274414\n",
    "# The length of relevance vetor is : 9\n",
    "# The running time for SVM is 0.015553712844848633\n",
    "# The length of support vetor is : 295\n",
    "\n",
    "#N=600\n",
    "#N=700\n",
    "#N=800      # 2.68s\n",
    "#N=900\n",
    "# N=1000      # 4.22\n",
    "\n",
    "X,y = generate_data(N, 0.2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = np.loadtxt('synth.tr.txt')\n",
    "\n",
    "X=np.zeros((Train.shape[0],2))\n",
    "y=np.zeros((Train.shape[0],1))\n",
    "\n",
    "X[:,0]=Train[:,0]\n",
    "X[:,1]=Train[:,1]\n",
    "y=Train[:,2]\n",
    "random.seed(5)\n",
    "idx = np.random.choice(250,100)\n",
    "X = X[idx]\n",
    "y = y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "#                           random_state=1, n_clusters_per_class=1)\n",
    "#X, Y = make_gaussian_quantiles(mean =[-2,2], cov=[1],n_features=2, n_classes=2, random_state=1)\n",
    "#X,y= make_circles(noise=0.02, factor=0.8, random_state=1)\n",
    "print(\"X shape is\",X.shape)\n",
    "print(\"y shape is\",y.shape)\n",
    "#rng = np.random.RandomState(2)\n",
    "#X += 2 * rng.uniform(size=X.shape)\n",
    "#linearly_separable = (X, y)\n",
    "#print(X)\n",
    "rvc_rbf=RVC(kernel='rbf',rbf_gamma=10,threshold_alpha=1e7)\n",
    "y_rvc = rvc_rbf.fit(X, y).predict(X)\n",
    "print(\"y_rvc\",y_rvc)\n",
    "print(\"y_rvc\",y_rvc[:, 1])\n",
    "print(\"y_rvc has the shape of\",y_rvc.shape)\n",
    "res = np.empty(y.shape[0])\n",
    "res[y_rvc[:, 1] <= 0.5] = 0\n",
    "res[y_rvc[:, 1] >= 0.5] = 1\n",
    "print(res)\n",
    "#print(\"deviation is\",1/np.sqrt(rvc_rbf.beta_))\n",
    "MSE_rvc = mean_squared_error(y, res)\n",
    "#MSE_rvc=np.sum(np.abs(y-y_rvc))\n",
    "#print(np.abs(y-y_rvc))\n",
    "print(\"MSE is\",MSE_rvc)\n",
    "print(\"The  length of relevance vector is:\",rvc_rbf.relevance_.shape[0])\n",
    "plt.figure()\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
    "plt.figure()\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=y_rvc,cmap=cm_bright,edgecolors='k')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=res,edgecolors='k')\n",
    "plt.scatter(rvc_rbf.relevance_[:, 0], rvc_rbf.relevance_[:, 1],marker=\"s\",c='y', edgecolors='k')\n",
    "#print(\"y orignal is\",y)\n",
    "#print(\"y predict is \",y_rvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rbf kernel\n",
    "X_test,y_test = generate_data(N, 0.2)\n",
    "start = time.time()\n",
    "#rvr_lin_spline = RVR(kernel='linear_spline')\n",
    "rvr_rbf = RVR(kernel='rbf',threshold_alpha=1e10,rbf_gamma=0.05)\n",
    "#svr_rbf = SVR(kernel='rbf')\n",
    "y_rvr = rvr_rbf.fit(X,y).predict(X_test)\n",
    "#y_svr = svr_rbf.fit(X,y).predict(X)\n",
    "#y_rvr_lin_spline=rvr_lin_spline.fit(X,y).predict(X_test)\n",
    "end = time.time()\n",
    "MSE_rvr = mean_squared_error(y_test, y_rvr)\n",
    "print('MSE for RVM is',MSE_rvr)\n",
    "print('The running time for RVM is',end-start)\n",
    "print(\"The length of relevance vetor is : {}\".format(len(rvr_lin_spline.relevance_)))\n",
    "# linear kernel\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "svr_rbf = SVR(kernel='rbf',gamma=0.05)\n",
    "#y_rvr = rvr_rbf.fit(X,y).predict(X)\n",
    "y_svr = svr_rbf.fit(X,y).predict(X_test)\n",
    "end = time.time()\n",
    "print('The running time for SVM is',end-start)\n",
    "print(\"The length of support vetor is : {}\".format(len(svr_rbf.support_vectors_)))\n",
    "\n",
    "\n",
    "#vr_lin = RVR(kernel='linear')\n",
    "#y_rvr = rvr_lin.fit(X, y).predict(X)\n",
    "#svr_lin = SVR(kernel='linear')\n",
    "#y_svr = svr_lin.fit(X, y).predict(X)\n",
    "\n",
    "# linear kernel\n",
    "\n",
    "\n",
    "# print(\"estimated sigma is : {}\".format(1/np.sqrt(rvr_rbf.beta_)))\n",
    "# print(\"The length of relevance vetor is : {}\".format(len(rvr_rbf.relevance_)))\n",
    "# plt.figure()\n",
    "# lw=2\n",
    "# plt.scatter(X, y, color='darkorange', label='data')\n",
    "# plt.plot(X,np.sin(X)/X,'b--',color='r')\n",
    "# #plt.plot(X, y_rvr, 'r-',color='navy', label='RBF Model')\n",
    "# #plt.plot(X, y_svr, 'r-',color='navy', lw=lw, label='Linear Spline Model')\n",
    "# plt.plot(X, y_rvr_lin_spline, 'r-',color='navy', lw=lw, label='Linear Spline Model')\n",
    "# #plt.scatter(rvr_rbf.relevance_, np.sin(rvr_rbf.relevance_)/rvr_rbf.relevance_, color='navy', label='Relevance vector')\n",
    "# #plt.scatter(rvr_lin_spline.relevance_, np.sin(rvr_lin_spline.relevance_)/rvr_lin_spline.relevance_, color='navy', label='Relevance vector')\n",
    "# plt.xlabel('data')\n",
    "# plt.ylabel('target')\n",
    "# plt.title('Relevance Vector Regression')\n",
    "# #plt.legend()\n",
    "# plt.figure()\n",
    "# plt.plot(X,np.sin(X)/X,'b--',color='r')\n",
    "# plt.scatter(X, y, color='darkorange', label='data')\n",
    "# plt.plot(X, y_rvr, 'r-',color='navy', label='RBF Model')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"The length of support vetor is : {}\".format(len(svr_lin.support_vectors_)))\n",
    "# plt.figure()\n",
    "# lw=2\n",
    "# #plt.scatter(X, y, color='darkorange', label='data')\n",
    "# plt.plot(X, y_svr, color='navy', lw=lw, label='RBF model')\n",
    "# plt.scatter(svr_lin.support_vectors_, 2 * svr_lin.support_vectors_ + 10,color='navy', label='Support vector')\n",
    "# plt.xlabel('data')\n",
    "# plt.ylabel('target')\n",
    "# plt.title('Support Vector Regression')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of relevance vetor is : 488\n",
      "The length of relevance vetor is : 9\n",
      "RMS for SVM is : 5.003661865062167\n",
      "RMS for RVM is : 4.784697682102961\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "#X, y = make_friedman1()\n",
    "#X, y = make_friedman2()\n",
    "#X, y = make_friedman3()\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# SVR and RVR fit\n",
    "rvr_lin = RVR(kernel='linear',threshold_alpha=1e10) # for friedman3, Boston\n",
    "y_rvr = rvr_lin.fit(X, y).predict(X)\n",
    "svr_lin = SVR(kernel='linear')\n",
    "y_svr = svr_lin.fit(X, y).predict(X)\n",
    "MSE_rvr = np.sqrt(mean_squared_error(y, y_rvr))\n",
    "MSE_svr = np.sqrt(mean_squared_error(y, y_svr))\n",
    "\n",
    "print(\"The length of relevance vetor is : {}\".format(len(svr_lin.support_vectors_)))\n",
    "print(\"The length of relevance vetor is : {}\".format(len(rvr_lin.relevance_)))\n",
    "print(\"RMS for SVM is : {}\".format(MSE_svr))\n",
    "print(\"RMS for RVM is : {}\".format(MSE_rvr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rbf kernel\n",
    "\n",
    "print(\"estimated sigma is : {}\".format(1/np.sqrt(rvr_rbf.beta_)))\n",
    "print(\"The length of relevance vetor is : {}\".format(len(rvr_rbf.relevance_)))\n",
    "plt.figure()\n",
    "lw=2\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_rvr, color='navy', lw=lw, label='RBF model')\n",
    "plt.scatter(rvr_rbf.relevance_, np.sin(rvr_rbf.relevance_)/rvr_rbf.relevance_, color='navy', label='Relevance vector')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Relevance Vector Regression')\n",
    "#plt.legend()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"The length of relevance vetor is : {}\".format(len(svr_rbf.support_vectors_)))\n",
    "plt.figure()\n",
    "lw=2\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_svr, color='navy', lw=lw, label='RBF model')\n",
    "plt.scatter(svr_rbf.support_vectors_, np.sin(svr_rbf.support_vectors_)/svr_rbf.support_vectors_,color='navy', label='Support vector')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_friedman3()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=np.array([[3,5,2]]).reshape((-1,1))\n",
    "y=np.array([[6,1,4]]).reshape((-1,1))\n",
    "print(x.shape)\n",
    "a = np.repeat(x,3,axis=1)\n",
    "b=np.repeat(y.T,3,axis=0)\n",
    "#print(a)\n",
    "#print(x)\n",
    "print(linear_kernel(x,y)) # np.dot(x,y.T)\n",
    "#print(np.dot(x,y.T))\n",
    "#Min= np.minimum(a,b)\n",
    "#print(Min)\n",
    "#print(np.dot(x.T,y))\n",
    "#print(np.dot(x.T,y)*Min)\n",
    "Min= np.minimum(a,b)\n",
    "print(Min)\n",
    "phi=1 + linear_kernel(x,y) + linear_kernel(x,y)*Min - (x+y.T)*np.square(Min)/2 + np.power(Min,3)\n",
    "#np.dot((x+y.T),np.square(Min))\n",
    "print(phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "X = np.sort(10 * np.random.rand(100, 1)-5, axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# #############################################################################\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - np.random.rand(20))\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "y_lin = svr_lin.fit(X, y).predict(X)\n",
    "y_poly = svr_poly.fit(X, y).predict(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Look at the results\n",
    "lw = 2\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')\n",
    "plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')\n",
    "plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.normal(0,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
